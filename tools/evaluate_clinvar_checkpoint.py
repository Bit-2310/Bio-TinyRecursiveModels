#!/usr/bin/env python
"""
Evaluate a trained VariantTRM checkpoint on the ClinVar test split.

The script expects an `all_config.yaml` generated by pretrain.py and a matching
checkpoint file (e.g., `step_62`). It reports accuracy and ROC AUC for the
binary pathogenic vs benign classification task.
"""

from __future__ import annotations

import argparse
import os
import json
import sys
from pathlib import Path
from typing import List, Tuple
from contextlib import nullcontext

import numpy as np
import torch
from omegaconf import OmegaConf

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from pretrain import PretrainConfig, create_model
from puzzle_dataset import PuzzleDataset, PuzzleDatasetConfig
from dataset.common import PuzzleDatasetMetadata
from models.losses import IGNORE_LABEL_ID
from models.recursive_reasoning.trm import (
    TinyRecursiveReasoningModel_ACTV1Carry,
    TinyRecursiveReasoningModel_ACTV1InnerCarry,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--config",
        type=Path,
        required=True,
        help="Path to all_config.yaml saved alongside the checkpoint.",
    )
    parser.add_argument(
        "--checkpoint",
        type=Path,
        required=True,
        help="Checkpoint file produced by pretrain.py (e.g., step_62).",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="Device to run evaluation on (default: auto-select CUDA if available).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Override global batch size used during evaluation.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional path to save metrics as JSON.",
    )
    parser.add_argument(
        "--save-preds",
        type=Path,
        default=None,
        help="Optional path to save per-example predictions as JSON lines.",
    )
    return parser.parse_args()


def load_vocab(vocab_path: Path) -> Tuple[int, int]:
    with vocab_path.open("r") as f:
        vocab_items = json.load(f)
    vocab_dict = {token: idx for token, idx in vocab_items}
    return vocab_dict["LABEL_BENIGN"], vocab_dict["LABEL_PATHOGENIC"]


def compute_auc(labels: np.ndarray, scores: np.ndarray) -> float:
    # Mannâ€“Whitney U based AUC computation
    order = np.argsort(scores)
    ranks = np.empty_like(order)
    ranks[order] = np.arange(len(scores))

    pos = labels == 1
    neg = labels == 0
    n_pos = pos.sum()
    n_neg = neg.sum()
    if n_pos == 0 or n_neg == 0:
        return float("nan")

    auc = (ranks[pos].sum() - n_pos * (n_pos - 1) / 2) / (n_pos * n_neg)
    return float(auc)


def main() -> None:
    os.environ.setdefault("DISABLE_COMPILE", "1")

    args = parse_args()

    cfg = OmegaConf.load(args.config)
    config = PretrainConfig(**OmegaConf.to_container(cfg, resolve=True))
    config.load_checkpoint = str(args.checkpoint)
    config.eval_save_outputs = []

    if args.batch_size is not None:
        config.global_batch_size = args.batch_size

    vocab_path = Path(config.data_paths[0]) / "vocab.json"
    benign_id, pathogenic_id = load_vocab(vocab_path)

    metadata_path = Path(config.data_paths[0]) / "train" / "dataset.json"
    with metadata_path.open("r") as f:
        metadata = PuzzleDatasetMetadata(**json.load(f))

    if args.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
            use_gpu = True
        else:
            os.environ.setdefault("CUDA_VISIBLE_DEVICES", "")
            device = torch.device("cpu")
            use_gpu = False
    else:
        device = torch.device(args.device)
        if device.type == "cpu":
            os.environ.setdefault("CUDA_VISIBLE_DEVICES", "")
            use_gpu = False
        else:
            use_gpu = torch.cuda.is_available()
    if not use_gpu:
        config.load_checkpoint = None
    model, _, _ = create_model(config, metadata, rank=0, world_size=1)
    if use_gpu:
        model.to(device)
        model.eval()
    else:
        model.cpu()
        model.eval()
        state_dict = torch.load(args.checkpoint, map_location="cpu")
        model.load_state_dict(state_dict, strict=False)
    model.eval()

    dataset_paths = config.data_paths_test if len(config.data_paths_test) else config.data_paths
    dataset_cfg = PuzzleDatasetConfig(
        seed=config.seed,
        dataset_paths=dataset_paths,
        global_batch_size=config.global_batch_size,
        test_set_mode=True,
        epochs_per_iter=1,
        rank=0,
        num_replicas=1,
    )
    eval_dataset = PuzzleDataset(dataset_cfg, split="test")

    all_labels: List[int] = []
    all_preds: List[int] = []
    all_scores: List[float] = []
    all_variants: List[int] = []

    if use_gpu:
        cuda_arg: str | int
        cuda_arg = device.index if device.index is not None else device.type
        device_ctx = torch.cuda.device(cuda_arg)
    else:
        device_ctx = nullcontext()

    with torch.no_grad():
        for _set_name, batch, _global_batch in eval_dataset:
            if use_gpu:
                batch = {k: v.to(device) for k, v in batch.items()}
                with device_ctx:
                    carry = model.initial_carry(batch)
                carry = _move_carry_to_device(carry, device)
            else:
                batch = {k: v.cpu() for k, v in batch.items()}
                carry = model.initial_carry(batch)

            while True:
                carry, _loss, _metrics, outputs, done = model(
                    carry=carry, batch=batch, return_keys=["logits"]
                )
                if done:
                    break

            logits = outputs["logits"][:, -1, :]
            labels = batch["labels"][:, -1]
            mask = labels != IGNORE_LABEL_ID

            if mask.any():
                logits = logits[mask]
                labels = labels[mask].to(torch.long)
                probs = torch.softmax(logits, dim=-1)

                all_labels.extend(labels.cpu().tolist())
                all_preds.extend(logits.argmax(dim=-1).cpu().tolist())
                all_scores.extend(probs[:, pathogenic_id].cpu().tolist())
                all_variants.extend(batch["puzzle_identifiers"][mask].cpu().tolist())

    all_labels = np.array(all_labels, dtype=np.int64)
    all_preds = np.array(all_preds, dtype=np.int64)
    all_scores = np.array(all_scores, dtype=np.float64)

    accuracy = float((all_preds == all_labels).mean()) if len(all_labels) else float("nan")
    binary_labels = (all_labels == pathogenic_id).astype(np.int64)
    auc = compute_auc(binary_labels, all_scores) if len(all_scores) else float("nan")

    metrics = {
        "accuracy": accuracy,
        "roc_auc": auc,
        "num_examples": int(len(all_labels)),
        "benign_token_id": benign_id,
        "pathogenic_token_id": pathogenic_id,
    }

    print(json.dumps(metrics, indent=2))

    if args.output is not None:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with args.output.open("w") as f:
            json.dump(metrics, f, indent=2)

    if args.save_preds is not None:
        args.save_preds.parent.mkdir(parents=True, exist_ok=True)
        with args.save_preds.open("w") as f:
            for variant_id, label, pred, score in zip(all_variants, all_labels, all_preds, all_scores):
                label_token = int(label)
                pred_token = int(pred)
                record = {
                    "variant_id": int(variant_id),
                    "label": int(label == pathogenic_id),
                    "pred": int(pred == pathogenic_id),
                    "score": float(score),
                    "label_token_id": label_token,
                    "pred_token_id": pred_token,
                }
                f.write(json.dumps(record) + "\n")


def _move_carry_to_device(
    carry: TinyRecursiveReasoningModel_ACTV1Carry,
    device: torch.device,
) -> TinyRecursiveReasoningModel_ACTV1Carry:
    carry.inner_carry.z_H = carry.inner_carry.z_H.to(device)
    carry.inner_carry.z_L = carry.inner_carry.z_L.to(device)
    carry.steps = carry.steps.to(device)
    carry.halted = carry.halted.to(device)
    carry.current_data = {k: v.to(device) for k, v in carry.current_data.items()}
    return carry


if __name__ == "__main__":
    main()
