#!/usr/bin/env python
"""
Evaluate a trained VariantTRM checkpoint on the ClinVar test split.

The script expects an `all_config.yaml` generated by pretrain.py and a matching
checkpoint file (e.g., `step_62`). It reports accuracy and ROC AUC for the
binary pathogenic vs benign classification task.
"""

from __future__ import annotations

import argparse
import os
import json
from pathlib import Path
from typing import List, Tuple

import numpy as np
import torch
from omegaconf import OmegaConf

from pretrain import PretrainConfig, create_model
from puzzle_dataset import PuzzleDataset, PuzzleDatasetConfig
from dataset.common import PuzzleDatasetMetadata
from models.losses import IGNORE_LABEL_ID


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--config",
        type=Path,
        required=True,
        help="Path to all_config.yaml saved alongside the checkpoint.",
    )
    parser.add_argument(
        "--checkpoint",
        type=Path,
        required=True,
        help="Checkpoint file produced by pretrain.py (e.g., step_62).",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="Device to run evaluation on (default: auto-select CUDA if available).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Override global batch size used during evaluation.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional path to save metrics as JSON.",
    )
    return parser.parse_args()


def load_vocab(vocab_path: Path) -> Tuple[int, int]:
    with vocab_path.open("r") as f:
        vocab_items = json.load(f)
    vocab_dict = {token: idx for token, idx in vocab_items}
    return vocab_dict["LABEL_BENIGN"], vocab_dict["LABEL_PATHOGENIC"]


def compute_auc(labels: np.ndarray, scores: np.ndarray) -> float:
    # Mannâ€“Whitney U based AUC computation
    order = np.argsort(scores)
    ranks = np.empty_like(order)
    ranks[order] = np.arange(len(scores))

    pos = labels == 1
    neg = labels == 0
    n_pos = pos.sum()
    n_neg = neg.sum()
    if n_pos == 0 or n_neg == 0:
        return float("nan")

    auc = (ranks[pos].sum() - n_pos * (n_pos - 1) / 2) / (n_pos * n_neg)
    return float(auc)


def main() -> None:
    os.environ.setdefault("DISABLE_COMPILE", "1")

    args = parse_args()

    cfg = OmegaConf.load(args.config)
    config = PretrainConfig(**OmegaConf.to_container(cfg, resolve=True))
    config.load_checkpoint = str(args.checkpoint)
    config.eval_save_outputs = []

    if args.batch_size is not None:
        config.global_batch_size = args.batch_size

    vocab_path = Path(config.data_paths[0]) / "vocab.json"
    benign_id, pathogenic_id = load_vocab(vocab_path)

    metadata_path = Path(config.data_paths[0]) / "train" / "dataset.json"
    with metadata_path.open("r") as f:
        metadata = PuzzleDatasetMetadata(**json.load(f))

    device = (
        torch.device("cuda")
        if args.device == "auto" and torch.cuda.is_available()
        else torch.device(args.device if args.device != "auto" else "cpu")
    )

    model, _, _ = create_model(config, metadata, rank=0, world_size=1)
    model.to(device)
    model.eval()

    dataset_paths = config.data_paths_test if len(config.data_paths_test) else config.data_paths
    dataset_cfg = PuzzleDatasetConfig(
        seed=config.seed,
        dataset_paths=dataset_paths,
        global_batch_size=config.global_batch_size,
        test_set_mode=True,
        epochs_per_iter=1,
        rank=0,
        num_replicas=1,
    )
    eval_dataset = PuzzleDataset(dataset_cfg, split="test")

    all_labels: List[int] = []
    all_preds: List[int] = []
    all_scores: List[float] = []

    with torch.no_grad():
        for _set_name, batch, _global_batch in eval_dataset:
            batch = {k: v.to(device) for k, v in batch.items()}
            carry = model.initial_carry(batch)

            while True:
                carry, _loss, _metrics, outputs, done = model(
                    carry=carry, batch=batch, return_keys=["logits"]
                )
                if done:
                    break

            logits = outputs["logits"][:, -1, :]
            labels = batch["labels"][:, -1]
            mask = labels != IGNORE_LABEL_ID

            if mask.any():
                logits = logits[mask]
                labels = labels[mask].to(torch.long)
                probs = torch.softmax(logits, dim=-1)

                all_labels.extend(labels.cpu().tolist())
                all_preds.extend(logits.argmax(dim=-1).cpu().tolist())
                all_scores.extend(probs[:, pathogenic_id].cpu().tolist())

    all_labels = np.array(all_labels, dtype=np.int64)
    all_preds = np.array(all_preds, dtype=np.int64)
    all_scores = np.array(all_scores, dtype=np.float64)

    accuracy = float((all_preds == all_labels).mean()) if len(all_labels) else float("nan")
    binary_labels = (all_labels == pathogenic_id).astype(np.int64)
    auc = compute_auc(binary_labels, all_scores) if len(all_scores) else float("nan")

    metrics = {
        "accuracy": accuracy,
        "roc_auc": auc,
        "num_examples": int(len(all_labels)),
        "benign_token_id": benign_id,
        "pathogenic_token_id": pathogenic_id,
    }

    print(json.dumps(metrics, indent=2))

    if args.output is not None:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with args.output.open("w") as f:
            json.dump(metrics, f, indent=2)


if __name__ == "__main__":
    main()
